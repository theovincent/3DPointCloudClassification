{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RangeNet++_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7HusUseMdwtac9wzDX1j+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theovincent/3DPointCloudClassification/blob/rangenet/range_net/RangeNet%2B%2B_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up"
      ],
      "metadata": {
        "id": "vwgAWsVAmR9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the RangeNet++ repository and install the dependencies"
      ],
      "metadata": {
        "id": "TOwlors0Qk9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMO09iGxOhCT",
        "outputId": "41857c3c-c9c7-4f46-a022-76ce5a154514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lidar-bonnetal'...\n",
            "remote: Enumerating objects: 167, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 167 (delta 46), reused 52 (delta 30), pack-reused 99\u001b[K\n",
            "Receiving objects: 100% (167/167), 17.45 MiB | 10.64 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/theovincent/lidar-bonnetal.git -b adapt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "\n",
        "os.chdir(\"/content/lidar-bonnetal/train/\")\n",
        "\n",
        "! pip install -r requirements.txt\n",
        "\n",
        "os.chdir(\"/content/lidar-bonnetal/train/tasks/semantic\")"
      ],
      "metadata": {
        "id": "IOmtipu6PltX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c083e460-0da8-4dee-8a1c-dbd577204467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.4.1)\n",
            "Collecting torch==1.1.0\n",
            "  Downloading torch-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (676.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 676.9 MB 3.8 kB/s \n",
            "\u001b[?25hCollecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████▍           | 328.1 MB 1.4 MB/s eta 0:02:19"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount the drive"
      ],
      "metadata": {
        "id": "MwozmBz_Qt69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/lidar-bonnetal/train/tasks/semantic/mount')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W4c7L3XzMk8",
        "outputId": "b96dcbcb-62ac-46c0-8c86-511175346ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/lidar-bonnetal/train/tasks/semantic/mount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the 3DPointCloudClassification repository and install the dependencies"
      ],
      "metadata": {
        "id": "KIdc1wqTQ0Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/')\n",
        "! git clone https://github.com/theovincent/3DPointCloudClassification.git -b rangenet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O0SuZ-pQQwW",
        "outputId": "9ea227d8-f2c3-4fa8-e2ea-87b31dd0a821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '3DPointCloudClassification'...\n",
            "remote: Enumerating objects: 248, done.\u001b[K\n",
            "remote: Counting objects: 100% (248/248), done.\u001b[K\n",
            "remote: Compressing objects: 100% (176/176), done.\u001b[K\n",
            "remote: Total 248 (delta 130), reused 177 (delta 66), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (248/248), 267.16 KiB | 10.69 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/3DPointCloudClassification')\n",
        "! pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqln4jLHRApo",
        "outputId": "8bc16401-cae3-4d3f-80ea-de692aaa34b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/3DPointCloudClassification\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from 3DPointCloudClassification==0.1) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from 3DPointCloudClassification==0.1) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from 3DPointCloudClassification==0.1) (4.63.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from 3DPointCloudClassification==0.1) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from 3DPointCloudClassification==0.1) (2.2.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->3DPointCloudClassification==0.1) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->3DPointCloudClassification==0.1) (1.3.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib->3DPointCloudClassification==0.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->3DPointCloudClassification==0.1) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->3DPointCloudClassification==0.1) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->3DPointCloudClassification==0.1) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->3DPointCloudClassification==0.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->3DPointCloudClassification==0.1) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->3DPointCloudClassification==0.1) (1.4.1)\n",
            "Installing collected packages: 3DPointCloudClassification\n",
            "  Attempting uninstall: 3DPointCloudClassification\n",
            "    Found existing installation: 3DPointCloudClassification 0.1\n",
            "    Can't uninstall '3DPointCloudClassification'. No files were found to uninstall.\n",
            "  Running setup.py develop for 3DPointCloudClassification\n",
            "Successfully installed 3DPointCloudClassification-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the dataset"
      ],
      "metadata": {
        "id": "xkI4co99RKgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the files from Drive"
      ],
      "metadata": {
        "id": "U9-1eLQBXvyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/lidar-bonnetal/train/tasks/semantic\")\n",
        "\n",
        "! cp -r mount/MyDrive/MVA/3DPointCloud/RangeNet++/data data"
      ],
      "metadata": {
        "id": "_lBYGSvrm5_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataset"
      ],
      "metadata": {
        "id": "_HVqZIX_XyCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/lidar-bonnetal/train/tasks/semantic\")\n",
        "from range_net import CITY_INFERANCE_FOLDER\n",
        "\n",
        "file = \"MiniLille1\"\n",
        "folder = CITY_INFERANCE_FOLDER[file]\n",
        "\n",
        "# !! CHANGE -itd OR 'nothing'\n",
        "! create_dataset -f $file -itd -ns 1 -pts data_city/sequences/FOLDER/velodyne\n",
        "! cp -r data_city/sequences/$folder data_city/sequences/08  # To fake a validation set\n",
        "! cp -r data_city/sequences/$folder data_city/sequences/11  # To fake a test set\n",
        "! create_dataset -f $file -itd -ns 10 -si -pts data_city/sequences/FOLDER/velodyne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgKwFMsRV6Oo",
        "outputId": "2b92223d-76a1-4c46-a30f-91070597c93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file': 'MiniLille1', 'is_train_data': True, 'n_samples': 1, 'path_to_store': 'data_city/sequences/FOLDER/velodyne', 'store_indexes': False, 'store_ply': False}\n",
            "100% 1/1 [00:00<00:00,  2.36it/s]\n",
            "{'file': 'MiniLille1', 'is_train_data': True, 'n_samples': 10, 'path_to_store': 'data_city/sequences/FOLDER/velodyne', 'store_indexes': True, 'store_ply': False}\n",
            "100% 10/10 [00:04<00:00,  2.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the weights"
      ],
      "metadata": {
        "id": "ZjxvmimOpd7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/lidar-bonnetal/train/tasks/semantic\")\n",
        "\n",
        "! cp -r mount/MyDrive/MVA/3DPointCloud/RangeNet++/model_1000 ."
      ],
      "metadata": {
        "id": "X9IqVgcNpZpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "InfYb1ztpDgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/3DPointCloudClassification')\n",
        "! git pull\n",
        "os.chdir('/content/lidar-bonnetal/train/tasks/semantic')\n",
        "! git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwblUAvQ4B3e",
        "outputId": "4e154f40-89df-4208-f235-9f525c17d259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)   \rUnpacking objects:  50% (2/4)   \rUnpacking objects:  75% (3/4)   \rUnpacking objects: 100% (4/4)   \rUnpacking objects: 100% (4/4), done.\n",
            "From https://github.com/theovincent/3DPointCloudClassification\n",
            "   1b59a8a..252abe1  rangenet   -> origin/rangenet\n",
            "Updating 1b59a8a..252abe1\n",
            "Fast-forward\n",
            " range_net/create_dataset.py | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/lidar-bonnetal/train/tasks/semantic')\n",
        "\n",
        "! ./infer.py --dataset data_city/ --log preds --model model_1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk29XakNpFzC",
        "outputId": "aa5059c6-0ffb-4896-8adb-315498419a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "INTERFACE:\n",
            "dataset data_city/\n",
            "log preds\n",
            "model model_1000\n",
            "----------\n",
            "\n",
            "Commit hash (training version):  b'756bf91'\n",
            "----------\n",
            "\n",
            "Opening arch config file from model_1000\n",
            "Opening data config file from model_1000\n",
            "train 00\n",
            "train 01\n",
            "train 02\n",
            "train 03\n",
            "train 04\n",
            "train 05\n",
            "train 06\n",
            "train 07\n",
            "train 09\n",
            "train 10\n",
            "valid 08\n",
            "test 11\n",
            "test 12\n",
            "test 13\n",
            "test 14\n",
            "test 15\n",
            "test 16\n",
            "test 17\n",
            "test 18\n",
            "test 19\n",
            "test 20\n",
            "test 21\n",
            "model folder exists! Using model from model_1000\n",
            "Sequences folder exists! Using sequences from data_city/sequences\n",
            "parsing seq 00\n",
            "parsing seq 01\n",
            "parsing seq 02\n",
            "parsing seq 03\n",
            "parsing seq 04\n",
            "parsing seq 05\n",
            "parsing seq 06\n",
            "parsing seq 07\n",
            "parsing seq 09\n",
            "parsing seq 10\n",
            "Using 10 scans from sequences [0, 1, 2, 3, 4, 5, 6, 7, 9, 10]\n",
            "Sequences folder exists! Using sequences from data_city/sequences\n",
            "parsing seq 08\n",
            "Using 1 scans from sequences [8]\n",
            "Sequences folder exists! Using sequences from data_city/sequences\n",
            "parsing seq 11\n",
            "parsing seq 12\n",
            "parsing seq 13\n",
            "parsing seq 14\n",
            "parsing seq 15\n",
            "parsing seq 16\n",
            "parsing seq 17\n",
            "parsing seq 18\n",
            "parsing seq 19\n",
            "parsing seq 20\n",
            "parsing seq 21\n",
            "Using 1 scans from sequences [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
            "Using DarknetNet53 Backbone\n",
            "Depth of backbone input =  5\n",
            "Original OS:  32\n",
            "New OS:  32\n",
            "Strides:  [2, 2, 2, 2, 2]\n",
            "Decoder original OS:  32\n",
            "Decoder new OS:  32\n",
            "Decoder strides:  [2, 2, 2, 2, 2]\n",
            "Total number of parameters:  50377364\n",
            "Total number of parameters requires_grad:  50377364\n",
            "Param encoder  40585504\n",
            "Param decoder  9786080\n",
            "Param head  5780\n",
            "Successfully loaded model backbone weights\n",
            "Successfully loaded model decoder weights\n",
            "Successfully loaded model head weights\n",
            "********************************************************************************\n",
            "Cleaning point-clouds with kNN post-processing\n",
            "kNN parameters:\n",
            "knn: 5\n",
            "search: 5\n",
            "sigma: 1.0\n",
            "cutoff: 1.0\n",
            "nclasses: 20\n",
            "********************************************************************************\n",
            "Infering in device:  cpu\n",
            "Infered seq 00 scan 000.label in 8.54657793045044 sec\n",
            "Infered seq 00 scan 001.label in 7.473333358764648 sec\n",
            "Infered seq 00 scan 002.label in 7.919509172439575 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"./infer.py\", line 109, in <module>\n",
            "    user.infer()\n",
            "  File \"../../tasks/semantic/modules/user.py\", line 78, in infer\n",
            "    to_orig_fn=self.parser.to_original)\n",
            "  File \"../../tasks/semantic/modules/user.py\", line 121, in infer_subset\n",
            "    proj_output = self.model(proj_in, proj_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"../../tasks/semantic/modules/segmentator.py\", line 150, in forward\n",
            "    y = self.decoder(y, skips)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"../..//tasks/semantic/decoders/darknet.py\", line 122, in forward\n",
            "    x, skips, os = self.run_layer(x, self.dec5, skips, os)\n",
            "  File \"../..//tasks/semantic/decoders/darknet.py\", line 111, in run_layer\n",
            "    feats = layer(x)  # up\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 92, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"../..//tasks/semantic/decoders/darknet.py\", line 28, in forward\n",
            "    out = self.conv2(out)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 338, in forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge labels"
      ],
      "metadata": {
        "id": "qBjPVysAN90T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! merge_labels -f $file -itd -pp preds/sequences/FOLDER/predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv-42oXs9yYQ",
        "outputId": "ff2ba2ce-92ad-40f1-fb4c-8ce86bf5483f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file': 'MiniLille1', 'is_train_data': True, 'path_predictions': 'preds/sequences/FOLDER/predictions'}\n",
            "100% 3/3 [00:00<00:00,  4.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save labels"
      ],
      "metadata": {
        "id": "M476ZMp5_nbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !! CHANGE train OR test\n",
        "! cp data/train/$file\\_with_range_net_* mount/MyDrive/MVA/3DPointCloud/RangeNet++/data/train/"
      ],
      "metadata": {
        "id": "hkZkFmE5-8pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls data/train/$file\\_with_range_net_*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnmC9Ubm-zcN",
        "outputId": "3120eae7-0efd-4337-b0f8-31ce35fae221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/train/MiniLille1_with_range_net_3_samples.ply\n"
          ]
        }
      ]
    }
  ]
}